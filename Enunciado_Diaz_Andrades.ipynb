{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2019 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Máquinas de Aprendizaje </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Introducción a librerías comunes de *Machine Learning*:\n",
    "    * Pandas\n",
    "    * Numpy\n",
    "    * Sklearn\n",
    "    * Matplotlib\n",
    "    * Otro..\n",
    "* Implementación de Perceptrón y variantes.\n",
    "* Implementación de método aprendizaje online (Gradiente descendente).\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega: 6 de Septiembre.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF393-II-2019]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Perceptrón a mano\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Perceptrón a mano\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"40%\"  />\n",
    "\n",
    "En esta sección se le pedirá que implemente el algoritmo online del *perceptrón* [[2]](#refs) para aprender una función de separación lineal en un problema de clasificación binaria (0 o 1) a través de la función de *treshold*. Un algoritmo online, como el caso del *perceptrón*, aprende de una instancia de dato a la vez $(x^{(i)},y^{(i)})$, dentro de un conjunto de datos $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\ldots, (x^{(N)},y^{(N)})  \\}$, donde la predicción de la clase para cada instancia es través de la función de *treshold*:\n",
    "\n",
    "$$\n",
    "\\hat{y_i} = f(x^{(i)};w,b) = \\left\\{ \\begin{array}{lc}\n",
    "       1 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b \\geq \\theta \\\\\n",
    "       0 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b < \\theta\n",
    "     \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Donde $\\theta = 0$. Recordar que el *bias* $b$ se puede incluir dentro de los pesos/parámetros $w$ si se agrega una columna extra de 1's a los datos de entrada $x$ (*como se ve en la imagen anterior*). \n",
    "\n",
    "Para lo que sigue de la actividad sólo podrá utilizar *numpy* (para operaciones de algebra lineal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from csv import reader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a) Escriba una función que calcule el valor de salida (*output*) del modelo $f(x)$ para un patrón de entrada $x$ a través de los pesos $w$ del modelo. *Decida si incluir los bias dentro de $w$ o manejarlos de manera separada*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref[5]\n",
    "def predict(row, weights):\n",
    "    activation = 0\n",
    "    for i in range(len(row)):\n",
    "        activation += weights[i] * row[i] #activation será un valor mayor a cero o (menor o igual) a cero.\n",
    "    return 1.0 if activation >= 0.0 else 0.0 #se retorna 1 o 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluye el bias en weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Escriba una función que implemente el clásico algoritmo del **Perceptrón** para un problema binario que permita entrenarlo en un conjunto de datos de tamaño $N$, leídos de manera *online* (uno a uno). *Recordar la decisión anterior sobre los bias*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref[5]\n",
    "def perceptron(datos, output, l_rate, n_pasada):\n",
    "    weights = [0.0 for i in range(len(datos[0]))]\n",
    "    for pasada in range(n_pasada):\n",
    "        sum_error = 0.0\n",
    "        cont = 0\n",
    "        for row in datos:\n",
    "            prediction = predict(row, weights)\n",
    "            error = output[cont] - prediction #se ve si coinciden o no. \n",
    "            cont += 1\n",
    "            sum_error += error**2 #sum_error aumentará solamente cuando output[cont] y prediction no coincidan\n",
    "            for i in range(len(row)):\n",
    "                #funcion del perceptron que entrena a los datos\n",
    "                weights[i] = weights[i] + l_rate*error*row[i] #solo cambia si output[cont] y prediction no coinciden\n",
    "        print('>error=%.3f' % (sum_error))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > c) Demuestre que lo implementado funciona en un problema real de clasificación. Para esto utilice el dataset **Breast cancer wisconsin**, disponible a través de la librería __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a la detección de cancer mamario a través de características relevantes (numéricas continuas) de un examen realizado, como por ejemplo la textura, simetría y tamaño de una masa mamaria. Estas características deben combinarse linealmente para la detección del cancer.\n",
    "> <div class=\"alert alert-block alert-info\">Es una buena práctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "\n",
    "``` python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X_train,y_train = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_train = np.c_[X_train, np.ones(N) ] #add columns of 1's if you want\n",
    "```\n",
    "\n",
    "Para evaluar los resultados mida la exactitud (*accuracy*) de la clasificación durante el entrenamiento (por cada iteración/instancia/dato) y grafique, utilice el conjunto de entrenamiento realizando una sola pasada (el objetivo de esta sección es familiarizarse con el algoritmo). Además reporte el tiempo de entrenamiento mediante el algoritmo implementado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">error=32.000\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "#load_csv(\"/anaconda3/lib/python3.6/site-packages/sklearn/datasets/data/breast_cancer.csv\")\n",
    "\n",
    "#X_train: matriz con rows con datos e y_train: matriz con resultados que debiese entregar predict\n",
    "X_train, y_train = load_breast_cancer(return_X_y = True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "N = len(X_train) #number of columns or number of elements by row.\n",
    "X_train = np.c_[X_train, np.ones(N)] #add columns of 1's if you want\n",
    "d = len(X_train[0]) #number of rows\n",
    "\n",
    "#learning rate a elección\n",
    "l_rate = 0.1\n",
    "#numero de pasadas que pide enunciado\n",
    "n_pasada = 1\n",
    "\n",
    "#imprime cantidad errores mientras entrena\n",
    "weights = perceptron(X_train,y_train, l_rate, n_pasada)\n",
    "\n",
    "\n",
    "cont2 = 0\n",
    "sum_numero = 0\n",
    "#veces en que se equivoca con pesos entrenados\n",
    "for row in X_train:\n",
    "    prediction = predict(row,weights)\n",
    "    numero = y_train[cont2] - prediction\n",
    "    cont2 += 1\n",
    "    sum_numero += numero**2\n",
    "print(sum_numero)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d) Escriba una función que implemente el **Forgetrón** [[3]](#refs) con una memoria de tamaño $K$ y la función de kernel como el producto interno (*inner-product*), esto es $<a,b> = \\sum_i a_i \\cdot b_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multvect(vector1, vector2):\n",
    "    return np.dot(vector1, vector2)\n",
    "\n",
    "def multimultvect(alpha, activeset, row):\n",
    "    sumatotal = 0\n",
    "    for i in range(len(alpha) - 1):\n",
    "        res = alpha[i] * multvect(activeset[i], row)\n",
    "        sumatotal += res\n",
    "    return sumatotal\n",
    "\n",
    "def listporesc(lista, escalar):\n",
    "    for i in range(len(lista)):\n",
    "        lista[i] = escalar*lista[i]\n",
    "    return lista\n",
    "\n",
    "\n",
    "def forgetron(dataset, output, K):\n",
    "    alpha = [] #lista de valores para cada elemento de activeset\n",
    "    activeset = [] #se agrega cada row que esta mal clasificado\n",
    "    activeset.append(dataset[0])\n",
    "    if (output[0] == 0):\n",
    "        alpha.append(-1)\n",
    "    else:\n",
    "        alpha.append(output[0]) #output[0] = 1\n",
    "    \n",
    "    cont3 = 1\n",
    "    for row in dataset[1:]:\n",
    "        #print(len(activeset), len(alpha))\n",
    "        pred = multimultvect(alpha, activeset, row)\n",
    "        if (output[cont3] == 0):\n",
    "            total = pred*(-1)\n",
    "        else:\n",
    "            total = pred*output[cont3]        \n",
    "    \n",
    "        if (total <= 0):\n",
    "            if len(activeset) < K:\n",
    "                activeset.append(row)\n",
    "                alpha = listporesc(alpha, 0.7)\n",
    "                if (output[cont3] == 0):\n",
    "                    alpha.append(-1)\n",
    "                else:\n",
    "                    alpha.append(output[cont3])\n",
    "            else:\n",
    "                activeset.pop(0)\n",
    "                alpha.pop(0)\n",
    "                activeset.append(row)\n",
    "                alpha = listporesc(alpha, 0.7)\n",
    "                if (output[cont3] == 0):\n",
    "                    alpha.append(-1)\n",
    "                    \n",
    "                else:\n",
    "                    alpha.append(output[cont3])\n",
    "                 \n",
    "        cont3 += 1\n",
    "    \n",
    "    return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Vuelva a realizar el item c) para el **Forgetrón** con un $K=10$ y compare los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04035360699999998,\n",
       " -0.05764800999999997,\n",
       " 0.08235429999999996,\n",
       " -0.11764899999999995,\n",
       " -0.16806999999999994,\n",
       " 0.24009999999999992,\n",
       " 0.3429999999999999,\n",
       " 0.48999999999999994,\n",
       " -0.7,\n",
       " -1]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forgetron(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué sucede al variar la función objetivo del problema? \n",
    "Si utilizáramos la función de pérdida *binary cross entropy*, que castiga de manera suave los valores en que se equivoca el modelo a través de que el valor de salida sea una confiabilidad $g(x; w,b) \\in [0,1]$.\n",
    "$$\n",
    "\\ell (y, \\ g(x;w,b)) = - y \\cdot \\log{(g(x;w,b))} - (1-y) \\cdot \\log{(1-g(x;w,b))}\n",
    "$$\n",
    "\n",
    "Realice una modificación al perceptrón para que entregue como salida una confiabilidad continua entre 0 y 1. Una buena aproximación de la función *treshold* (con $\\theta=0$) del perceptrón es la función sigmoidal.\n",
    "\n",
    "<img src=\"https://i.imgur.com/lr6F3Ur.png\" width=\"60%\"  />\n",
    "\n",
    "Ésto sería modelar el perceptrón como:\n",
    "$$\n",
    "g(x^{(i)};w,b) = p(y=1|x^{(i)}) = \\sigma \\left( \\sum_j w_j \\cdot x^{(i)}_j +b \\right)\n",
    "$$\n",
    "\n",
    "Con $\\sigma$ la función sigmoidal de la forma $\\sigma(\\xi) = 1/(1+e^{-\\xi}) $, la cual tiene una derivada cíclica que hace más fácil el cálculo: $\\sigma'(\\xi) = \\sigma(\\xi) (1-\\sigma(\\xi))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> f) Escriba una función que compute la función sigmoidal para una entrada $\\xi$ cualquiera. *Tenga cuidado con los límites de números que puede trabajar python (por ejemplo $\\exp{800}\\rightarrow +\\infty$)*. *Se aconseja acotar/truncar los valores que entran a la función para que la operación se pueda realizar*. Además escriba una función que calcule la salida del nuevo modelo $g(x; w,b)$ con esta función sigmoidal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidal(exp):\n",
    "    if exp >= 710:\n",
    "        exp = 709\n",
    "    elif exp <= -746:\n",
    "        exp = -745\n",
    "    \n",
    "    return (1 / (1 + math.exp(-exp)))\n",
    "\n",
    "def g(equis, doblev):\n",
    "    return sigmoidal(multvect(equis,doblev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> g) Escriba una función que calcule la función de pérdida descrita anteriormente para un dato $x^{(i)}$, utilizando $g(x^{(i)};w,b)$. *Tenga cuidado con los límites del logaritmo (recordad que $\\log{0}\\rightarrow +\\infty$)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.328310311951279e-05"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#−𝑦⋅log(𝑔(𝑥;𝑤,𝑏))−(1−𝑦)⋅log(1−𝑔(𝑥;𝑤,𝑏))\n",
    "def funcion(X_train,weigths,index):\n",
    "    return(g(X_train[index],weigths))\n",
    "\n",
    "funcion(X_train,weights,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> h) Escriba una función que calcule el gradiente (derivada) de la función de pérdida anterior con respecto a los pesos del modelo $w$. *Se recomienda derivarla analíticamente y luego escribirla*. *Recuerde el uso de la regla de la cadena*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de los correspondientes despejes se llegó a la expresión:\n",
    "    $$ Gradiente = (g - y)\\vec{X} $$\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(x_train,y_train,weights,index):\n",
    "    numero = g(x_train[index],weights) - y_train[index]\n",
    "    return listporesc(x_train[index],numero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i) Realice una modificación al algoritmo implementado en b) (**Perceptrón**) para que se adapte a la función objetivo *binary cross entropy* implementada, para ésto haga uso del algoritmo de optimización SGD [[4]](#refs) (*Stochastic Gradient Descend*) con tasa de aprendizaje $\\eta \\in [0,1]$.\n",
    "\n",
    "$$ \\vec{w}^{(t+1)} \\leftarrow \\vec{w}^{(t)} - \\eta \\cdot \\nabla_{\\vec{w}^{(t)}} \\ell $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron2(datos, output, l_rate, n_pasada):\n",
    "    weights = [0.0 for i in range(len(datos[0]))]\n",
    "    for pasada in range(n_pasada):\n",
    "        sum_error = 0.0\n",
    "        cont = 0\n",
    "        for row in datos:\n",
    "            prediction = predict(row, weights)\n",
    "            error = output[cont] - prediction #se ve si coinciden o no. \n",
    "\n",
    "            sum_error += error**2 #sum_error aumentará solamente cuando output[cont] y prediction no coincidan\n",
    "            grad = gradiente(datos,output,weights,cont)\n",
    "            for i in range(len(row)):\n",
    "                #funcion del perceptron que entrena a los datos\n",
    "                weights[i] = weights[i] - l_rate*grad[i] #solo cambia si output[cont] y prediction no coinciden\n",
    "            cont += 1\n",
    "        print('>error=%.3f' % (sum_error))\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> j) Vuelva a realizar el item c) con esta modificación, además grafique la función de pérdida en el transcurso del entrenamiento. Compare los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">error=26.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.6460775337319291,\n",
       " -0.4524962066610003,\n",
       " -0.6316362365860246,\n",
       " -0.6684732954133146,\n",
       " -0.16384825961014834,\n",
       " -0.032355980241376564,\n",
       " -0.5334435977443787,\n",
       " -0.7185551747685259,\n",
       " -0.15970208766877517,\n",
       " 0.36877962262329933,\n",
       " -0.877825412035712,\n",
       " 0.059336200464646056,\n",
       " -0.7654271372538797,\n",
       " -0.7653112284540272,\n",
       " -0.2660331008343406,\n",
       " 0.35468965971884386,\n",
       " 0.28168634455150066,\n",
       " -0.23078828791923456,\n",
       " 0.07110553957027123,\n",
       " 0.28671082119779656,\n",
       " -0.8530682886726081,\n",
       " -0.6293079673706091,\n",
       " -0.8330623620006405,\n",
       " -0.8554771945152855,\n",
       " -0.5072393037920231,\n",
       " -0.13653480382621572,\n",
       " -0.39539422401355356,\n",
       " -0.7432845800264163,\n",
       " -0.6189272880296892,\n",
       " -0.09763621693260957,\n",
       " 0.5365128319212641]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos = perceptron2(X_train,y_train, l_rate, n_pasada)\n",
    "pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "### Referencias\n",
    "[1] Hastie, T.; Tibshirani, R., Friedman, J. (2009), *The Elements of Statistical Learning*, Second Edition.\n",
    "Springer New York Inc.  \n",
    "[2] STEPHEN, I. (1990). *Perceptron-based learning algorithms*. IEEE Transactions on neural networks, 50(2), 179.  \n",
    "[3] Dekel, O., Shalev-Shwartz, S., & Singer, Y. (2006). *The Forgetron: A kernel-based perceptron on a fixed budget*. In Advances in neural information processing systems (pp. 259-266).  \n",
    "[4] Ruder, S. (2016). *An overview of gradient descent optimization algorithms*. arXiv preprint arXiv:1609.04747.\n",
    "\n",
    "[5] Para mayor entendimiento de los incisos a) y b), basamos nuestra respuesta con ayuda de: https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/#targetText=In%20machine%20learning%2C%20we%20can,model%20one%20at%20a%20time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
