{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 M√°quinas de Aprendizaje II-2019 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducci√≥n a M√°quinas de Aprendizaje </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Introducci√≥n a librer√≠as comunes de *Machine Learning*:\n",
    "    * Pandas\n",
    "    * Numpy\n",
    "    * Sklearn\n",
    "    * Matplotlib\n",
    "    * Otro..\n",
    "* Implementaci√≥n de Perceptr√≥n y variantes.\n",
    "* Implementaci√≥n de m√©todo aprendizaje online (Gradiente descendente).\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega: 6 de Septiembre.\n",
    "* Formato de entrega: envƒ±ÃÅo de link Github al correo electr√≥nico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF393-II-2019]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Perceptr√≥n a mano\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Perceptr√≥n a mano\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"40%\"  />\n",
    "\n",
    "En esta secci√≥n se le pedir√° que implemente el algoritmo online del *perceptr√≥n* [[2]](#refs) para aprender una funci√≥n de separaci√≥n lineal en un problema de clasificaci√≥n binaria (0 o 1) a trav√©s de la funci√≥n de *treshold*. Un algoritmo online, como el caso del *perceptr√≥n*, aprende de una instancia de dato a la vez $(x^{(i)},y^{(i)})$, dentro de un conjunto de datos $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\ldots, (x^{(N)},y^{(N)})  \\}$, donde la predicci√≥n de la clase para cada instancia es trav√©s de la funci√≥n de *treshold*:\n",
    "\n",
    "$$\n",
    "\\hat{y_i} = f(x^{(i)};w,b) = \\left\\{ \\begin{array}{lc}\n",
    "       1 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b \\geq \\theta \\\\\n",
    "       0 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b < \\theta\n",
    "     \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Donde $\\theta = 0$. Recordar que el *bias* $b$ se puede incluir dentro de los pesos/par√°metros $w$ si se agrega una columna extra de 1's a los datos de entrada $x$ (*como se ve en la imagen anterior*). \n",
    "\n",
    "Para lo que sigue de la actividad s√≥lo podr√° utilizar *numpy* (para operaciones de algebra lineal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from csv import reader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a) Escriba una funci√≥n que calcule el valor de salida (*output*) del modelo $f(x)$ para un patr√≥n de entrada $x$ a trav√©s de los pesos $w$ del modelo. *Decida si incluir los bias dentro de $w$ o manejarlos de manera separada*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref[5]\n",
    "def predict(row, weights):\n",
    "    activation = 0\n",
    "    for i in range(len(row)):\n",
    "        activation += weights[i] * row[i] #activation ser√° un valor mayor a cero o (menor o igual) a cero.\n",
    "    return 1.0 if activation >= 0.0 else 0.0 #se retorna 1 o 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluye el bias en weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> b) Escriba una funci√≥n que implemente el cl√°sico algoritmo del **Perceptr√≥n** para un problema binario que permita entrenarlo en un conjunto de datos de tama√±o $N$, le√≠dos de manera *online* (uno a uno). *Recordar la decisi√≥n anterior sobre los bias*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ref[5]\n",
    "def perceptron(datos, output, l_rate, n_pasada):\n",
    "    weights = [0.0 for i in range(len(datos[0]))]\n",
    "    for pasada in range(n_pasada):\n",
    "        sum_error = 0.0\n",
    "        cont = 0\n",
    "        for row in datos:\n",
    "            prediction = predict(row, weights)\n",
    "            error = output[cont] - prediction #se ve si coinciden o no. \n",
    "            cont += 1\n",
    "            sum_error += error**2 #sum_error aumentar√° solamente cuando output[cont] y prediction no coincidan\n",
    "            for i in range(len(row)):\n",
    "                #funcion del perceptron que entrena a los datos\n",
    "                weights[i] = weights[i] + l_rate*error*row[i] #solo cambia si output[cont] y prediction no coinciden\n",
    "        print('>error=%.3f' % (sum_error))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > c) Demuestre que lo implementado funciona en un problema real de clasificaci√≥n. Para esto utilice el dataset **Breast cancer wisconsin**, disponible a trav√©s de la librer√≠a __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a la detecci√≥n de cancer mamario a trav√©s de caracter√≠sticas relevantes (num√©ricas continuas) de un examen realizado, como por ejemplo la textura, simetr√≠a y tama√±o de una masa mamaria. Estas caracter√≠sticas deben combinarse linealmente para la detecci√≥n del cancer.\n",
    "> <div class=\"alert alert-block alert-info\">Es una buena pr√°ctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "\n",
    "``` python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X_train,y_train = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_train = np.c_[X_train, np.ones(N) ] #add columns of 1's if you want\n",
    "```\n",
    "\n",
    "Para evaluar los resultados mida la exactitud (*accuracy*) de la clasificaci√≥n durante el entrenamiento (por cada iteraci√≥n/instancia/dato) y grafique, utilice el conjunto de entrenamiento realizando una sola pasada (el objetivo de esta secci√≥n es familiarizarse con el algoritmo). Adem√°s reporte el tiempo de entrenamiento mediante el algoritmo implementado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">error=32.000\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "#load_csv(\"/anaconda3/lib/python3.6/site-packages/sklearn/datasets/data/breast_cancer.csv\")\n",
    "\n",
    "#X_train: matriz con rows con datos e y_train: matriz con resultados que debiese entregar predict\n",
    "X_train, y_train = load_breast_cancer(return_X_y = True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "N = len(X_train) #number of columns or number of elements by row.\n",
    "X_train = np.c_[X_train, np.ones(N)] #add columns of 1's if you want\n",
    "d = len(X_train[0]) #number of rows\n",
    "\n",
    "#learning rate a elecci√≥n\n",
    "l_rate = 0.1\n",
    "#numero de pasadas que pide enunciado\n",
    "n_pasada = 1\n",
    "\n",
    "#imprime cantidad errores mientras entrena\n",
    "weights = perceptron(X_train,y_train, l_rate, n_pasada)\n",
    "\n",
    "\n",
    "cont2 = 0\n",
    "sum_numero = 0\n",
    "#veces en que se equivoca con pesos entrenados\n",
    "for row in X_train:\n",
    "    prediction = predict(row,weights)\n",
    "    numero = y_train[cont2] - prediction\n",
    "    cont2 += 1\n",
    "    sum_numero += numero**2\n",
    "print(sum_numero)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d) Escriba una funci√≥n que implemente el **Forgetr√≥n** [[3]](#refs) con una memoria de tama√±o $K$ y la funci√≥n de kernel como el producto interno (*inner-product*), esto es $<a,b> = \\sum_i a_i \\cdot b_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multvect(vector1, vector2):\n",
    "    return np.dot(vector1, vector2)\n",
    "\n",
    "def multimultvect(alpha, activeset, row):\n",
    "    sumatotal = 0\n",
    "    for i in range(len(alpha) - 1):\n",
    "        res = alpha[i] * multvect(activeset[i], row)\n",
    "        sumatotal += res\n",
    "    return sumatotal\n",
    "\n",
    "def listporesc(lista, escalar):\n",
    "    for i in range(len(lista)):\n",
    "        lista[i] = escalar*lista[i]\n",
    "    return lista\n",
    "\n",
    "\n",
    "def forgetron(dataset, output, K):\n",
    "    alpha = [] #lista de valores para cada elemento de activeset\n",
    "    activeset = [] #se agrega cada row que esta mal clasificado\n",
    "    activeset.append(dataset[0])\n",
    "    if (output[0] == 0):\n",
    "        alpha.append(-1)\n",
    "    else:\n",
    "        alpha.append(output[0]) #output[0] = 1\n",
    "    \n",
    "    cont3 = 1\n",
    "    for row in dataset[1:]:\n",
    "        #print(len(activeset), len(alpha))\n",
    "        pred = multimultvect(alpha, activeset, row)\n",
    "        if (output[cont3] == 0):\n",
    "            total = pred*(-1)\n",
    "        else:\n",
    "            total = pred*output[cont3]        \n",
    "    \n",
    "        if (total <= 0):\n",
    "            if len(activeset) < K:\n",
    "                activeset.append(row)\n",
    "                alpha = listporesc(alpha, 0.7)\n",
    "                if (output[cont3] == 0):\n",
    "                    alpha.append(-1)\n",
    "                else:\n",
    "                    alpha.append(output[cont3])\n",
    "            else:\n",
    "                activeset.pop(0)\n",
    "                alpha.pop(0)\n",
    "                activeset.append(row)\n",
    "                alpha = listporesc(alpha, 0.7)\n",
    "                if (output[cont3] == 0):\n",
    "                    alpha.append(-1)\n",
    "                    \n",
    "                else:\n",
    "                    alpha.append(output[cont3])\n",
    "                 \n",
    "        cont3 += 1\n",
    "    \n",
    "    return alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> e) Vuelva a realizar el item c) para el **Forgetr√≥n** con un $K=10$ y compare los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04035360699999998,\n",
       " -0.05764800999999997,\n",
       " 0.08235429999999996,\n",
       " -0.11764899999999995,\n",
       " -0.16806999999999994,\n",
       " 0.24009999999999992,\n",
       " 0.3429999999999999,\n",
       " 0.48999999999999994,\n",
       " -0.7,\n",
       " -1]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forgetron(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¬øQu√© sucede al variar la funci√≥n objetivo del problema? \n",
    "Si utiliz√°ramos la funci√≥n de p√©rdida *binary cross entropy*, que castiga de manera suave los valores en que se equivoca el modelo a trav√©s de que el valor de salida sea una confiabilidad $g(x; w,b) \\in [0,1]$.\n",
    "$$\n",
    "\\ell (y, \\ g(x;w,b)) = - y \\cdot \\log{(g(x;w,b))} - (1-y) \\cdot \\log{(1-g(x;w,b))}\n",
    "$$\n",
    "\n",
    "Realice una modificaci√≥n al perceptr√≥n para que entregue como salida una confiabilidad continua entre 0 y 1. Una buena aproximaci√≥n de la funci√≥n *treshold* (con $\\theta=0$) del perceptr√≥n es la funci√≥n sigmoidal.\n",
    "\n",
    "<img src=\"https://i.imgur.com/lr6F3Ur.png\" width=\"60%\"  />\n",
    "\n",
    "√âsto ser√≠a modelar el perceptr√≥n como:\n",
    "$$\n",
    "g(x^{(i)};w,b) = p(y=1|x^{(i)}) = \\sigma \\left( \\sum_j w_j \\cdot x^{(i)}_j +b \\right)\n",
    "$$\n",
    "\n",
    "Con $\\sigma$ la funci√≥n sigmoidal de la forma $\\sigma(\\xi) = 1/(1+e^{-\\xi}) $, la cual tiene una derivada c√≠clica que hace m√°s f√°cil el c√°lculo: $\\sigma'(\\xi) = \\sigma(\\xi) (1-\\sigma(\\xi))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> f) Escriba una funci√≥n que compute la funci√≥n sigmoidal para una entrada $\\xi$ cualquiera. *Tenga cuidado con los l√≠mites de n√∫meros que puede trabajar python (por ejemplo $\\exp{800}\\rightarrow +\\infty$)*. *Se aconseja acotar/truncar los valores que entran a la funci√≥n para que la operaci√≥n se pueda realizar*. Adem√°s escriba una funci√≥n que calcule la salida del nuevo modelo $g(x; w,b)$ con esta funci√≥n sigmoidal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidal(exp):\n",
    "    if exp >= 710:\n",
    "        exp = 709\n",
    "    elif exp <= -746:\n",
    "        exp = -745\n",
    "    \n",
    "    return (1 / (1 + math.exp(-exp)))\n",
    "\n",
    "def g(equis, doblev):\n",
    "    return sigmoidal(multvect(equis,doblev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> g) Escriba una funci√≥n que calcule la funci√≥n de p√©rdida descrita anteriormente para un dato $x^{(i)}$, utilizando $g(x^{(i)};w,b)$. *Tenga cuidado con los l√≠mites del logaritmo (recordad que $\\log{0}\\rightarrow +\\infty$)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.328310311951279e-05"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#‚àíùë¶‚ãÖlog(ùëî(ùë•;ùë§,ùëè))‚àí(1‚àíùë¶)‚ãÖlog(1‚àíùëî(ùë•;ùë§,ùëè))\n",
    "def funcion(X_train,weigths,index):\n",
    "    return(g(X_train[index],weigths))\n",
    "\n",
    "funcion(X_train,weights,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> h) Escriba una funci√≥n que calcule el gradiente (derivada) de la funci√≥n de p√©rdida anterior con respecto a los pesos del modelo $w$. *Se recomienda derivarla anal√≠ticamente y luego escribirla*. *Recuerde el uso de la regla de la cadena*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despu√©s de los correspondientes despejes se lleg√≥ a la expresi√≥n:\n",
    "    $$ Gradiente = (g - y)\\vec{X} $$\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente(x_train,y_train,weights,index):\n",
    "    numero = g(x_train[index],weights) - y_train[index]\n",
    "    return listporesc(x_train[index],numero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i) Realice una modificaci√≥n al algoritmo implementado en b) (**Perceptr√≥n**) para que se adapte a la funci√≥n objetivo *binary cross entropy* implementada, para √©sto haga uso del algoritmo de optimizaci√≥n SGD [[4]](#refs) (*Stochastic Gradient Descend*) con tasa de aprendizaje $\\eta \\in [0,1]$.\n",
    "\n",
    "$$ \\vec{w}^{(t+1)} \\leftarrow \\vec{w}^{(t)} - \\eta \\cdot \\nabla_{\\vec{w}^{(t)}} \\ell $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron2(datos, output, l_rate, n_pasada):\n",
    "    weights = [0.0 for i in range(len(datos[0]))]\n",
    "    for pasada in range(n_pasada):\n",
    "        sum_error = 0.0\n",
    "        cont = 0\n",
    "        for row in datos:\n",
    "            prediction = predict(row, weights)\n",
    "            error = output[cont] - prediction #se ve si coinciden o no. \n",
    "\n",
    "            sum_error += error**2 #sum_error aumentar√° solamente cuando output[cont] y prediction no coincidan\n",
    "            grad = gradiente(datos,output,weights,cont)\n",
    "            for i in range(len(row)):\n",
    "                #funcion del perceptron que entrena a los datos\n",
    "                weights[i] = weights[i] - l_rate*grad[i] #solo cambia si output[cont] y prediction no coinciden\n",
    "            cont += 1\n",
    "        print('>error=%.3f' % (sum_error))\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> j) Vuelva a realizar el item c) con esta modificaci√≥n, adem√°s grafique la funci√≥n de p√©rdida en el transcurso del entrenamiento. Compare los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">error=26.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.6460775337319291,\n",
       " -0.4524962066610003,\n",
       " -0.6316362365860246,\n",
       " -0.6684732954133146,\n",
       " -0.16384825961014834,\n",
       " -0.032355980241376564,\n",
       " -0.5334435977443787,\n",
       " -0.7185551747685259,\n",
       " -0.15970208766877517,\n",
       " 0.36877962262329933,\n",
       " -0.877825412035712,\n",
       " 0.059336200464646056,\n",
       " -0.7654271372538797,\n",
       " -0.7653112284540272,\n",
       " -0.2660331008343406,\n",
       " 0.35468965971884386,\n",
       " 0.28168634455150066,\n",
       " -0.23078828791923456,\n",
       " 0.07110553957027123,\n",
       " 0.28671082119779656,\n",
       " -0.8530682886726081,\n",
       " -0.6293079673706091,\n",
       " -0.8330623620006405,\n",
       " -0.8554771945152855,\n",
       " -0.5072393037920231,\n",
       " -0.13653480382621572,\n",
       " -0.39539422401355356,\n",
       " -0.7432845800264163,\n",
       " -0.6189272880296892,\n",
       " -0.09763621693260957,\n",
       " 0.5365128319212641]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pesos = perceptron2(X_train,y_train, l_rate, n_pasada)\n",
    "pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "### Referencias\n",
    "[1] Hastie, T.; Tibshirani, R., Friedman, J. (2009), *The Elements of Statistical Learning*, Second Edition.\n",
    "Springer New York Inc.  \n",
    "[2] STEPHEN, I. (1990). *Perceptron-based learning algorithms*. IEEE Transactions on neural networks, 50(2), 179.  \n",
    "[3] Dekel, O., Shalev-Shwartz, S., & Singer, Y. (2006). *The Forgetron: A kernel-based perceptron on a fixed budget*. In Advances in neural information processing systems (pp. 259-266).  \n",
    "[4] Ruder, S. (2016). *An overview of gradient descent optimization algorithms*. arXiv preprint arXiv:1609.04747.\n",
    "\n",
    "[5] Para mayor entendimiento de los incisos a) y b), basamos nuestra respuesta con ayuda de: https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/#targetText=In%20machine%20learning%2C%20we%20can,model%20one%20at%20a%20time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
